# AI Person â€” Bá»™ NÃ£o Thá»© 2

> **Personal Memory-First AI System** â€” Version 0.3.0

A production-grade personal AI that stores your thinking history and reasons over it. Not a chatbot. Not a RAG demo. A long-term memory infrastructure designed to live alongside you for 5â€“10 years.

---

## ğŸ— Architecture

```
API Layer (FastAPI)
    â†“
Memory Infrastructure   â†’   Retrieval Engine   â†’   Reasoning Layer
(save, checksum, embed)     (semantic search)       (mode + prompt + LLM)
    â†“                              â†“                       â†“
              PostgreSQL 16 + pgvector (HNSW)
```

**5 modes:** `RECALL` (fetch verbatim) Â· `SYNTHESIZE` (combine knowledge) Â· `REFLECT` (analyze evolution) Â· `CHALLENGE` (find contradictions) Â· `EXPAND` (supplement with external knowledge)

**LLM Provider:** OpenAI API hoáº·c **LM Studio** (local model) â€” chuyá»ƒn Ä‘á»•i qua 1 env var.

---

## âš¡ Quick Start

### 1. Prerequisites

- Python 3.11+
- Docker + Docker Compose
- **Má»™t trong hai:**
  - OpenAI API key, hoáº·c
  - [LM Studio](https://lmstudio.ai/) cháº¡y local model

### 2. Setup

```bash
# Clone and enter
cd AI_Person

# Create virtual environment
python -m venv venv
.\venv\Scripts\activate      # Windows
# source venv/bin/activate   # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Configure environment
copy .env.example .env
# Edit .env â€” xem hÆ°á»›ng dáº«n bÃªn dÆ°á»›i
```

### 3. Start Database

```bash
docker compose up -d
```

### 4. Run Migrations

```bash
alembic upgrade head
```

### 5. Start API Server

```bash
uvicorn app.main:app --reload --port 8000
```

### 6. Start Embedding Worker (separate terminal)

```bash
python -m workers.run_embedding
```

API docs: `http://localhost:8000/docs` (chá»‰ khi `DEBUG=true`)

---

## âš™ï¸ Cáº¥u HÃ¬nh LLM Provider

Há»‡ thá»‘ng há»— trá»£ 2 provider, chuyá»ƒn Ä‘á»•i qua biáº¿n `LLM_PROVIDER` trong `.env`.

### Option A: LM Studio (Local â€” Recommended cho dev)

```env
LLM_PROVIDER=lmstudio
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# Äá»•i tÃªn model cho khá»›p vá»›i model báº¡n load trong LM Studio
LLM_MODEL=your-chat-model-name
EMBEDDING_MODEL=your-embedding-model-name
EMBEDDING_DIMENSION=768                    # Äá»•i theo dimension model embedding
```

**CÃ¡ch setup LM Studio:**

1. Má»Ÿ LM Studio â†’ load **2 models**:
   - 1 **chat model** (vd: `llama-3.2-3b-instruct`, `qwen2.5-7b-instruct`)
   - 1 **embedding model** (vd: `nomic-embed-text-v1.5`, `bge-small-en-v1.5`)
2. Start server trong LM Studio (default port: `1234`)
3. Verify: `curl http://localhost:1234/v1/models` â€” pháº£i tháº¥y danh sÃ¡ch models
4. Copy tÃªn model chÃ­nh xÃ¡c vÃ o `LLM_MODEL` vÃ  `EMBEDDING_MODEL` trong `.env`

> âš ï¸ **EMBEDDING_DIMENSION** pháº£i match vá»›i model báº¡n dÃ¹ng:
> - `nomic-embed-text-v1.5` â†’ `768`
> - `bge-small-en-v1.5` â†’ `384`
> - `text-embedding-3-small` (OpenAI) â†’ `1536`

### Option B: OpenAI API

```env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-real-key-here

LLM_MODEL=gpt-4.1-mini
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536
```

---

## âš™ï¸ Táº¥t Cáº£ Environment Variables

| Variable | Default | Description |
|---|---|---|
| `DATABASE_URL` | `postgresql+asyncpg://...` | PostgreSQL connection string |
| `LLM_PROVIDER` | `openai` | `"openai"` hoáº·c `"lmstudio"` |
| `LMSTUDIO_BASE_URL` | `http://localhost:1234/v1` | LM Studio server URL |
| `OPENAI_API_KEY` | *(required náº¿u openai)* | OpenAI API key |
| `EMBEDDING_MODEL` | `text-embedding-3-small` | TÃªn embedding model |
| `EMBEDDING_DIMENSION` | `1536` | Vector dimension |
| `LLM_MODEL` | `gpt-4.1-mini` | TÃªn chat/LLM model |
| `MAX_CONTEXT_TOKENS` | `3000` | Token budget cho memory context |
| `LOG_LEVEL` | `INFO` | Log level |
| `DEBUG` | `false` | Enable Swagger UI + debug mode |
| `EMBEDDING_WORKER_INTERVAL_SECONDS` | `5` | Worker polling interval |
| `EMBEDDING_WORKER_BATCH_SIZE` | `10` | Batch size cho embedding worker |

---

## ğŸ”Œ API Endpoints

| Method | Path | Description |
|--------|------|-------------|
| `POST` | `/api/v1/memory` | Save a new memory |
| `GET` | `/api/v1/memory/{id}` | Get memory by ID |
| `PATCH` | `/api/v1/memory/{id}/archive` | Soft-archive (selective forgetting) |
| `POST` | `/api/v1/search` | Semantic search |
| `POST` | `/api/v1/query` | Reasoning query (5 modes) |
| `GET` | `/health` | Health check |

### CLI â€” Add Memory (Interactive)

```powershell
.\ai add
```

Interactive flow:
1. Nháº­p ná»™i dung (multiline, káº¿t thÃºc báº±ng `::end`)
2. Chá»n `content_type` (6 loáº¡i)
3. Flow ngÆ°á»i (person_name suggestion)
4. Chá»n tags (22 tags cá»‘ Ä‘á»‹nh)
5. XÃ¡c nháº­n â†’ lÆ°u qua `MemoryService`

### Chat UI â€” React (Vite)

```powershell
.\ai chat
```

Má»Ÿ browser táº¡i `http://localhost:5173`. Giao diá»‡n gá»“m 3 tab:
- **Chat** â€” TrÃ² chuyá»‡n vá»›i AI, chá»n 1 trong 5 mode (RECALL, SYNTHESIZE, REFLECT, CHALLENGE, EXPAND)
- **Memory** â€” ThÃªm memory má»›i (form) + tra cá»©u theo ID
- **Search** â€” Semantic search vá»›i filter (content_type, threshold, limit)

### Save Memory

```bash
curl -X POST http://localhost:8000/api/v1/memory \
  -H "Content-Type: application/json" \
  -d '{
    "raw_text": "HÃ´m nay tÃ´i nháº­n ra ráº±ng momentum trong ML khÃ´ng chá»‰ lÃ  ká»¹ thuáº­t mÃ  lÃ  tÆ° duy.",
    "content_type": "reflection",
    "importance_score": 0.8,
    "metadata": {
      "tags": ["ai", "deep"],
      "source": "api"
    }
  }'
```

### Search

```bash
curl -X POST http://localhost:8000/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "suy nghÄ© vá» machine learning",
    "limit": 10
  }'
```

### Reasoning Query (5 modes)

```bash
# RECALL â€” tráº£ nguyÃªn vÄƒn
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Tao tá»«ng viáº¿t gÃ¬ vá» LoRA?", "mode": "RECALL"}'

# SYNTHESIZE â€” tá»•ng há»£p tá»« nhiá»u memory
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Tá»•ng há»£p hiá»ƒu biáº¿t vá» fine-tuning", "mode": "SYNTHESIZE"}'

# REFLECT â€” phÃ¢n tÃ­ch evolution
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query": "TÆ° duy cá»§a tao vá» AI thay Ä‘á»•i tháº¿ nÃ o?", "mode": "REFLECT"}'

# CHALLENGE â€” pháº£n biá»‡n
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query": "TÃ¬m mÃ¢u thuáº«n trong suy nghÄ© vá» ML cá»§a tao", "mode": "CHALLENGE"}'

# EXPAND â€” bá»• sung external knowledge
curl -X POST http://localhost:8000/api/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query": "So sÃ¡nh hiá»ƒu biáº¿t cá»§a tao vá»›i industry best practice", "mode": "EXPAND"}'
```

---

## ğŸ—‚ Project Structure

```
AI_Person/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/              # HTTP endpoints (no business logic)
â”‚   â”œâ”€â”€ core/             # Personality, prompts, token guard
â”‚   â”œâ”€â”€ db/               # SQLAlchemy models + Alembic migrations
â”‚   â”œâ”€â”€ exceptions/       # Custom exception classes + handlers
â”‚   â”œâ”€â”€ llm/              # LLM + Embedding adapters (OpenAI + LM Studio)
â”‚   â”œâ”€â”€ logging/          # Structured JSON logger
â”‚   â”œâ”€â”€ memory/           # Save + checksum + embedding job creation
â”‚   â”œâ”€â”€ reasoning/        # Mode controller + prompt builder + orchestrator
â”‚   â”œâ”€â”€ retrieval/        # Semantic search + ranking
â”‚   â”œâ”€â”€ schemas/          # Pydantic request/response models
â”‚   â”œâ”€â”€ config.py         # Settings (all env vars)
â”‚   â”œâ”€â”€ deps.py           # DI factory (provider switching)
â”‚   â””â”€â”€ main.py           # FastAPI app entry point
â”œâ”€â”€ cli/                  # Interactive CLI ingestion
â”‚   â”œâ”€â”€ add_memory.py     # `ai add` â€” main interactive flow
â”‚   â”œâ”€â”€ registry.py       # Content type, tag, type menus
â”‚   â””â”€â”€ person_helpers.py # Person name suggest + normalize
â”œâ”€â”€ workers/
â”‚   â””â”€â”€ run_embedding.py  # Background embedding CLI
â”œâ”€â”€ personalities/
â”‚   â””â”€â”€ default.yaml      # AI personality config (5-mode hints)
â”œâ”€â”€ docs/                 # Architecture documentation
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

---

## ğŸŒ¿ Git Workflow

```
main                    â† production-ready
â”œâ”€â”€ develop             â† integration
â”‚   â”œâ”€â”€ feat/lmstudio-adapter
â”‚   â”œâ”€â”€ fix/audit-gaps
â”‚   â””â”€â”€ feature/phase1â€“5
```

---

## ğŸ“š Documentation

| Doc | Description |
|---|---|
| [MEMORY_CONTRACT.md](docs/MEMORY_CONTRACT.md) | **Memory Contract V1** â€” data schema, tag registry, examples |
| [PROJECT_STRUCTURE.md](docs/PROJECT_STRUCTURE.md) | Architecture philosophy + data flow |
| [DATA_DESIGN.md](docs/DATA_DESIGN.md) | DB schema, indexes, retrieval SQL |
| [CODEBASE_STRUCTURE.md](docs/CODEBASE_STRUCTURE.md) | File responsibilities |
| [IMPLEMENTATION_PLAN.md](docs/IMPLEMENTATION_PLAN.md) | Phase roadmap + checklists |
| [API_DOCS.md](docs/API_DOCS.md) | Full API reference (endpoints, schemas, errors, cURL) |

---

## ğŸ“¦ Version

Current: **v0.3.0** â€” Memory Contract V1 + Reasoning Layer safety fixes + LM Studio support
