# TODO â€” AI Person (Bá»™ NÃ£o Thá»© 2)

> **Version:** v0.3.0
> **Last Updated:** 2026-02-21
> **Status:** Memory Contract V1 + CLI V1 + Reasoning safety â†’ E2E test

---

## ðŸ”¥ P0 â€” v0.3.0 Completed

- [x] **Memory Contract V1:**
  - [x] `content_type` â†’ 6 fixed values (note, conversation, reflection, idea, article, log)
  - [x] Drop `source_type` column â†’ `metadata.source` (migration 004)
  - [x] Create `docs/MEMORY_CONTRACT.md` â€” full spec with tag registry, examples
  - [x] Update all docs (DATA_DESIGN, API_DOCS, README, etc.)
- [x] **CLI V1 â€” Interactive `ai add`:**
  - [x] `cli/registry.py` â€” content_type, tag, type menus
  - [x] `cli/add_memory.py` â€” 9-step interactive flow
  - [x] `cli/person_helpers.py` â€” normalize + suggest
  - [x] `MemoryRepository.get_distinct_person_names()`
  - [x] `ai.ps1` â€” `.\ai add` command
- [x] **Reasoning Layer Safety Fixes:**
  - [x] RECALL `must_cite_memory_id = True` (was False)
  - [x] `validate_citations()` in ReasoningService â€” enforces `[Memory N]` format
  - [x] EXPAND "No Override" guard â€” memory is source of truth
  - [x] REFLECT "No Psychological Inference" guard
  - [x] Citation format instruction in PromptBuilder
  - [x] `PolicyViolationError` exception class
- [x] **ENUM â†’ VARCHAR migration** (003) â€” fixes asyncpg type mismatch

- [ ] **Setup `.env`** â€” copy `.env.example` â†’ `.env`, Ä‘iá»n `OPENAI_API_KEY`
- [x] **âœ… Fix REFLECT epistemic conflict:**
  - [x] `prompts.py`: Ä‘á»•i `REFLECT.can_use_external_knowledge = True` â†’ `False`
  - [x] `prompts.py`: xÃ³a external mention trong REFLECT instruction
  - [x] `service.py`: Ä‘á»•i sang EXPAND-only, xÃ³a `MIN_CONTEXT_TOKENS` + token-threshold
  - [x] `service.py`: thay báº±ng mode-based rule (`policy.can_use_external_knowledge`)
- [x] **âœ… Upgrade to 5-mode:**
  - [x] `prompts.py`: thÃªm SYNTHESIZE + EXPAND vÃ o `MODE_INSTRUCTIONS` + `MODE_POLICIES`
  - [x] `mode_controller.py`: `VALID_MODES` = 5 modes, raises `InvalidModeError`
  - [x] `schemas/query.py`: `ModeEnum` vá»›i 5 values + `content_type` validator
  - [x] `ranking.py`: 5-mode weights per DATA_DESIGN 7.2.1
  - [x] `exceptions/handlers.py`: `InvalidModeError` (422)
  - [x] `prompt_builder.py`: docstring updated to 5 modes
- [x] **Cháº¡y Docker** â€” `docker compose up -d` âœ…
- [x] **Cháº¡y migration** â€” `alembic upgrade head` âœ… â€” all 7 indexes created
  - `idx_memory_embedding` (HNSW)
  - `idx_memory_created_at`
  - `idx_memory_content_type`
  - `idx_memory_metadata` (GIN)
  - `idx_memory_checksum` (UNIQUE)
  - `idx_embedding_jobs_status`
  - `idx_memory_embedding_model`
- [ ] **End-to-end test thá»§ cÃ´ng:**
  - [ ] `POST /api/v1/memory` â†’ insert thÃ nh cÃ´ng
  - [ ] Check `embedding_jobs` status = `pending`
  - [ ] Cháº¡y worker: `python -m workers.run_embedding`
  - [ ] Verify `memory_records.embedding != NULL`
  - [ ] Check `embedding_jobs` status = `completed`
  - [ ] `POST /api/v1/search` â†’ tráº£ káº¿t quáº£ Ä‘Ãºng
  - [ ] `POST /api/v1/query` vá»›i `mode=RECALL` â†’ tra Ä‘Ãºng
  - [ ] `POST /api/v1/query` vá»›i `mode=SYNTHESIZE` â†’ tá»•ng há»£p memory
  - [ ] `POST /api/v1/query` vá»›i `mode=REFLECT` â†’ nháº­n diá»‡n evolution
  - [ ] `POST /api/v1/query` vá»›i `mode=CHALLENGE` â†’ verify `external_knowledge_used=false`
  - [ ] `POST /api/v1/query` vá»›i `mode=EXPAND` â†’ verify `external_knowledge_used=true`

---

## ðŸŸ¡ P1 â€” NÃªn lÃ m sá»›m

- [x] **5-Mode code migration** (completed)
  - [x] ðŸ”´ Update `_EXTERNAL_KNOWLEDGE_ALLOWED_MODES` â†’ `{"EXPAND"}` in `reasoning/service.py`
  - [x] ðŸ”´ Remove `MIN_CONTEXT_TOKENS` + token-threshold logic in `reasoning/service.py`
  - [x] ðŸ”´ Replace token-threshold conditional with `if mode == "EXPAND"` in `reasoning/service.py`
  - [x] Add SYNTHESIZE + EXPAND weights to `_MODE_WEIGHTS` in `retrieval/ranking.py`
  - [x] Update `personalities/default.yaml` with SYNTHESIZE + EXPAND prompts
  - [x] Implement `metadata_filter` â†’ SQL JSONB containment (`@>`) in `retrieval/search.py`
  - [x] Add `content_type` enum validation to `schemas/search.py`
  - [x] Add `INVALID_MODE` error to `exceptions/handlers.py`
- [ ] **Phase 0: Behavior Freeze**
  - [ ] Chá»‘t system prompt final trong `personalities/default.yaml`
  - [ ] Test 30 lÆ°á»£t chat tay, verify AI giá»¯ Ä‘Ãºng nhÃ¢n cÃ¡ch
  - [ ] Verify mode RECALL / REFLECT / CHALLENGE hoáº¡t Ä‘á»™ng Ä‘Ãºng
  - [ ] Document káº¿t quáº£ vÃ o `docs/BEHAVIOR_FREEZE.md`
- [ ] **Verify epistemic boundary thá»±c táº¿:**
  - [ ] REFLECT vá»›i 0 memory â†’ tráº£ "khÃ´ng cÃ³ memory liÃªn quan", external_knowledge_used=false 
  - [ ] REFLECT vá»›i nhiá»u memory dÃ i (>800 tokens) â†’ `external_knowledge_used=false`
- [ ] **Cáº­p nháº­t GitHub repo URLs trong `CHANGELOG.md`** (khi cÃ³ remote)
- [ ] **Cáº­p nháº­t Project Context** trong `PROMPT.md` skill:
  - `Project name:` AI Person â€” Bá»™ NÃ£o Thá»© 2
  - `Current version:` v0.1.1
  - `Tech stack:` FastAPI, SQLAlchemy 2.0, asyncpg, pgvector, OpenAI
  - `Current progress:` All phases complete, pending first real run

---

## ðŸ”µ P2 â€” Backlog / V2

- [x] **`local_adapter.py`** â€” LM Studio local model adapter (via OpenAI-compatible API)
- [ ] **Summary persistence** â€” LLM-generated summary vá»›i user approval flow (V2)
  - `is_summary=true`, `metadata.parent_id`, `metadata.generated_by="system"`
  - Máº·c Ä‘á»‹nh excluded khá»i retrieval
- [ ] **Chunking tá»± Ä‘á»™ng** â€” auto-chunk PDF / article dÃ i trÆ°á»›c khi insert (V2)
- [ ] **Partition strategy** â€” khi > 1M records, partition `memory_records` theo thÃ¡ng
- [ ] **Re-embed pipeline** â€” khi Ä‘á»•i embedding model, re-embed toÃ n bá»™ records (V2)
- [ ] **Backup strategy** â€” cron daily backup, verify checksum integrity

---

## âœ… ÄÃ£ hoÃ n thÃ nh

- [x] **Phase 1: Foundation** â€” DB, ORM (3 tables + 7 indexes), migration, session, config, logging, exceptions
- [x] **Phase 2: Memory Infrastructure** â€” MemoryService, EmbeddingWorker, repository, embedding adapters
- [x] **Phase 3: Retrieval Engine** â€” RetrievalService, ranking formula (mode-aware), diversity guard, TokenGuard
- [x] **Phase 4: Reasoning Layer** â€” ReasoningService, ModeController, PromptBuilder, LLM adapters
- [x] **Phase 5: API Layer** â€” memory / search / query endpoints, deps DI, main.py, CORS, correlation ID middleware
- [x] **Documentation** â€” README.md, .env.example, docker-compose.yml, personalities/default.yaml
- [x] **Fix #1: Epistemic Boundary** â€” token-threshold (800) replaces count-based rule in code + docs
- [x] **Fix #2: Index count** â€” 5 â†’ 7, named in IMPLEMENTATION_PLAN checklist
- [x] **Fix #3: Migration path** â€” `alembic init alembic` corrected to `app/db/migrations` in DATA_DESIGN
- [x] **Fix #4: is_summary filter** â€” `AND is_summary = false` added to retrieval SQL in DATA_DESIGN
- [x] **Fix #5: Summary policy** â€” V1 Strict: LLM khÃ´ng persist, section 8.2 drop-only
- [x] **Worker concurrency** â€” `SELECT FOR UPDATE SKIP LOCKED` in `get_pending_jobs()`
- [x] **CHANGELOG.md** â€” created, v0.1.0 + v0.1.1 documented
- [x] **TODO.md** â€” this file
- [x] **API_DOCS.md** â€” full API reference (5 endpoints, schemas, modes, error codes, cURL examples)
- [x] **OpenClaw analysis** â€” applied doc recommendations (Memory-First Principle, engagement V2, etc.)
- [x] **5-Mode Design** â€” docs migration: RECALL/SYNTHESIZE/REFLECT/CHALLENGE/EXPAND
  - `idea.md`: full rewrite with 5-mode ideal + architecture flow
  - `PROJECT_STRUCTURE.md`: sections 2.1, 5, Epistemic Boundary, Policy Guard
  - `API_DOCS.md`: modes table, cURL examples
  - `IMPLEMENTATION_PLAN.md`: Phase 4 checklist + test scenarios
  - Retired: ANALYZE, TEMPORAL_COMPARE â†’ merged into SYNTHESIZE, REFLECT
  - Retired: token-threshold (800) â†’ mode-based (EXPAND = external ON)
- [x] **Fix #6: metadata_filter** â€” JSONB `@>` containment implemented in `retrieval/search.py`
- [x] **Fix #7: content_type validator** â€” `field_validator` added to `schemas/search.py`
- [x] **Fix #8: Personality mode_hints** â€” `personalities/default.yaml` updated with 5-mode focus + style hints
- [x] **LM Studio Adapter** â€” `lmstudio_adapter.py` + `lmstudio_embedding_adapter.py`, provider switch via `LLM_PROVIDER` env var
